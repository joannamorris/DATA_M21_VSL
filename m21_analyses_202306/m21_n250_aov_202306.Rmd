---
title: "M21 202306 n250 aov"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  message=FALSE, comment = "")
```

This R script contains the code for analysing the morph 21 erp data for the 200-300 ms time window.

First we load the libraries we need

```{r}
library(readr)
library(psych)
library(dplyr)
library(tidyr)
library(ggplot2)
```

Let's load the N250 erp data file and the spelling and  vocab data, then  we join the files. We will use the `inner_join` rather than the `full_join` function in order to eliminate rows with missing data.

```{r}
sv_202303.na <- read_csv("m21_spell_vocab_raw_z_pca.csv", show_col_types = FALSE)
n250 <- read_csv("S101-177_n250.csv", show_col_types = FALSE)
frq_nw <- read_csv("nw_frq_2.csv")
frq_cs <- read_csv("cw_frq_2.csv")
n250 <- inner_join(sv_202303.na,n250, by = "SubjID")  #join subject PCA data
```


Let's save a `.csv` file with the data from the combined dataset 

```{r}
write_csv(n250, "202303_sv_n250_rmna.csv")
```

Let's create separate dataframes for the words and non-words
```{r}

```



We will create a subset with only the electrode sites we will be 
analysing—F3, Fz, F4, C3, Cz, C4, P3, Pz, P4

```{r c12}
sites = c(3,2, 25, 7, 20, 21, 12, 11, 16)
n250_9 <- dplyr::filter(n250, chindex %in% sites)

```


We then create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use the`mutate` function from the dplyr package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r}
n250_9 <- dplyr::mutate(n250_9,
                        anteriority = case_when(grepl("F", chlabel) ~ "Frontal",
                                                grepl("C", chlabel) ~ "Central",
                                                grepl("P", chlabel) ~ "Parietal"))

n250_9 <- dplyr::mutate(n250_9,
                        laterality = case_when(grepl("3", chlabel) ~ "Left",
                                               grepl("z", chlabel) ~ "Midline",
                                               grepl("Z", chlabel) ~ "Midline",
                                               grepl("4", chlabel) ~ "Right"))



n250_9 <- dplyr::mutate(n250_9,
                        fam_size = case_when(grepl("small", binlabel) ~ "Small",
                                             grepl("large", binlabel) ~ "Large"))
```


We then create a smaller dataset with only the columns we need

```{r}
n250_9b <- dplyr::select(n250_9, 
                             SubjID, 
                             lang_type, 
                             anteriority, 
                             laterality, 
                             fam_size,
                             value,
                             chlabel,
                             binlabel)
```

We then divide dataset into 3 separate ones—for "words", "simple nonwords" and "complex nonwords"

```{r}
n250_words <- dplyr::filter(n250_9b, grepl("Critical_word",binlabel))
n250_nwsmpl <- dplyr::filter(n250_9b, grepl("simple",binlabel))
n250_nwcplx <- dplyr::filter(n250_9b, grepl("complex",binlabel))
```




#Plot Means

Get condition means

```{r get_means}

#Define standard error of the mean function

sem <- function(x) sd(x)/sqrt(length(x))

(cw.cond.means <- n250_words |> 
   group_by(fam_size, lang_type) |> 
   summarise(mean = mean(value), 
             se = sem(value),
             num_stim = n()))


(nw_smp.cond.means <- n250_nwsmpl |> 
    group_by(fam_size, lang_type) |> 
    summarise(mean = mean(value), 
              se = sem(value),
              num_stim = n()))

(nw_cpx.cond.means <- n250_nwcplx |> 
    group_by(fam_size, lang_type) |> 
    summarise(mean = mean(value), 
              se = sem(value),
              num_stim = n()))
```

Barplots

```{r c15, fig.height= 6, fig.width= 6, echo=FALSE}
library(gridExtra)
p1 <-  cw.cond.means |> ggplot(aes(x=lang_type, 
                                   y=mean, 
                                   fill = fam_size, 
                                   ymin = mean - se, 
                                   ymax = mean + se)) +
  coord_cartesian(xlim = NULL, 
                  ylim = c(-2, 2.2), 
                  expand = TRUE, 
                  default = FALSE,
                  clip = "on") +
  geom_col(position = "dodge", width = 0.5, color = "black")  +
  ylab("Voltage (microvolts)")  +  
  xlab("Participant Reading Style")  + 
  ggtitle("Complex Words") +
  scale_fill_manual(values = c("coral2", "deepskyblue3"))+ 
  geom_errorbar(width = .08, position = position_dodge(0.5)) + 
  theme_classic() + 
   geom_text(aes(label = round(mean, digits = 2)),
             colour = "black", 
             size = 2.5, 
             vjust = -4, 
             position = position_dodge(.5))+
  guides(fill=guide_legend(title="Morphological Family Size"))

p2 <-  nw_smp.cond.means |> ggplot(aes(x=lang_type, 
                                       y=mean, fill = fam_size, 
                                       ymin = mean - se, 
                                       ymax = mean + se)) +
  coord_cartesian(xlim = NULL, 
                  ylim = c(-2, 2.2), 
                  expand = TRUE, 
                  default = FALSE,
                  clip = "on") +
  geom_col(position = "dodge", width = .7, color = "black")  +
  xlab("")  + 
  ylab("Voltage (microvolts)")  +  
  ggtitle("Simple NonWords") +
  scale_fill_manual(values = c("coral2", "deepskyblue3"))+ 
  geom_errorbar(width = .08, position = position_dodge(0.7)) + 
  theme_classic() + 
   geom_text(aes(label = round(mean, digits = 2)),
             colour = "black", 
             size = 2.5, 
             vjust = -3.5, 
             position = position_dodge(.7)) +
  guides(fill=guide_legend(title="Morphological Family Size"))

p3 <-  nw_cpx.cond.means |> ggplot(aes(x=lang_type, 
                                       y=mean, 
                                       fill = fam_size, 
                                       ymin = mean - se, 
                                       ymax = mean + se)) +
  coord_cartesian(xlim = NULL, 
                  ylim = c(-2, 2.2), 
                  expand = TRUE, 
                  default = FALSE,
                  clip = "on") +
  geom_col(position = "dodge", width = .7, color = "black")  +
  xlab("Participant Reading Style")  + 
  ylab("Voltage (microvolts)")  +  
  ggtitle("Complex NonWords") +
  scale_fill_manual(values = c("coral2", "deepskyblue3"))+ 
  geom_errorbar(width = .08, position = position_dodge(0.7)) + 
  theme_classic() + 
   geom_text(aes(label = round(mean, digits = 2)),
             colour = "black", 
             size = 2.5, 
             vjust = -3.5,  
             position = position_dodge(.7)) +
  guides(fill=guide_legend(title="Morphological Family Size"))

#grid.arrange(p1, p2, p3)
grid.arrange(p2, p3)
```

10. Now we can compute the ANOVA for each of the three datasets.



```{r}
library(ez)

ezANOVA(data = n250_words,
        dv = value,
        wid = SubjID,
        within = .(fam_size, anteriority, laterality),
        within_full = .(fam_size, anteriority, laterality, chlabel),
        between = lang_type,
        type = 3)

ezANOVA(data = n250_nwsmpl,
        dv = value,
        wid = SubjID,
        within = .(fam_size, anteriority, laterality),
        within_full = .(fam_size, anteriority, laterality, chlabel),
        between = lang_type,
        type = 3)

ezANOVA(data = n250_nwcplx,
        dv = value,
        wid = SubjID,
        within = .(fam_size, anteriority, laterality),
        within_full = .(fam_size, anteriority, laterality, chlabel),
        between = lang_type,
        type = 3)
```

